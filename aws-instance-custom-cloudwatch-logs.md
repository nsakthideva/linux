#### aws-instance-custom-cloudwatch-logs.md

----

**Process**

Use cloudwatch agents when the nginx web server generates logs that are forwarded to the cloudwatch log group. If an error is generated by nginx, the lambda function should capture it and store it in an S3 bucket.

**Requirements**

| RESOURCE | INTEGRATION |
|---|---|
| ec2 instance | Required |
| lambda | Required |
| cloud watch | Required |
| s3 | Required |


**Action**

* Attaching an IAM Role to instance
* Install CloudWatch Logs Agent
* Configure CloudWatch Logs Agent

**IAM Role**

```json
{
    "Version": "2012-10-17",
    "Statement": [{
        "Effect": "Allow",
        "Action": [
            "logs:CreateLogGroup",
            "logs:CreateLogStream",
            "logs:PutLogEvents",
            "logs:DescribeLogStreams"
        ],
        "Resource": [
            "arn:aws:logs:*:*:*"
        ]
    }]
}

```

**Install CloudWatch Logs Agent**

```bash
sudo apt update
sudo apt install curl -y
curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
python3 ./awslogs-agent-setup.py --region ap-south-1
```

**Install nginx**

```bash
sudo apt update
sudo apt install nginx -y
sudo systemctl start nginx
sudo systemctl enable nginx
```

**How to trigger the nginx logs**


```bash
# open a terminal, connect to an SSH session, and run the command to generate logs.
for i in {1..1000}; do curl -o /dev/null -s -w "%{http_code}\n" http://locahost; sleep 2; done
```

```bash
# open a terminal, connect to an SSH session, and run the command to check access logs.
tail -f /var/log/nginx/access.log
```

**Configure CloudWatch Logs Agent**

Edit /var/awslogs/etc/awslogs.conf file which will have default log path 

```bash
[/var/log/nginx/access.log]
datetime_format = %d/%b/%Y:%H:%M:%S %z
file = /var/log/nginx/access.log
buffer_duration = 5000
log_stream_name = access.log
initial_position = end_of_file
log_group_name = /ec2/nginx/logs

[/var/log/nginx/error.log]
datetime_format = %Y/%m/%d %H:%M:%S
file = /var/log/nginx/error.log
buffer_duration = 5000
log_stream_name = error.log
initial_position = end_of_file
log_group_name = /ec2/nginx/logs
```

restart cloudwatch agent**

After making changes, restart the cloudwatch agent on the ecw instance.

```bash
sudo systemctl restart awslogs
```

**cloudwatch logs**

When logs are created, the ec2 instance's cloudwatch agent automatically pushes them to the cloudwatch logs group. we can get it from cloudwatch log group.

![image](https://user-images.githubusercontent.com/57703276/165018504-9b2d1781-c524-4262-ad86-1a4590945e86.png)

**cloudwatch stream**

![image](https://user-images.githubusercontent.com/57703276/165018765-48f54378-9925-4504-8767-2661c0b9a4c6.png)

**lambda function subscribtion**

create the lambda function to capture the error logs and sent to them into S3 bucket.

![image](https://user-images.githubusercontent.com/57703276/165019127-1e7e4c2a-6b4a-49fd-9081-ba349b5cfd8f.png)

**lambda fuction**

```py
import gzip
import json
import base64
import boto3

def lambda_handler(event, context):
    cw_data = event['awslogs']['data']
    compressed_payload = base64.b64decode(cw_data)
    uncompressed_payload = gzip.decompress(compressed_payload)
    payload = json.loads(uncompressed_payload)

    AWS_BUCKET_NAME = 'nginx-logs-store'
    s3 = boto3.resource('s3')
    data = payload
    object = s3.Object(AWS_BUCKET_NAME, 'results.json')
    object.put(
        Body=(bytes(json.dumps(payload).encode('UTF-8')))
    )
    
    log_events = payload['logEvents']
    for log_event in log_events:
        print(f'LogEvent: {log_event}')
```

**subscription filter**

Create a subscription filter in the cloudwatch log group and send events to lambda for processing.

![image](https://user-images.githubusercontent.com/57703276/165019641-c5136674-6dc6-4624-8d8c-c1589897ed2b.png)

**S3 datas**

Finally, you can examine the log in the S3 bucket.

![image](https://user-images.githubusercontent.com/57703276/165019341-20022b5d-3cb5-45ac-87f5-2c50c0f1af26.png)




